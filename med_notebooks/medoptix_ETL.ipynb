{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30cb864",
   "metadata": {},
   "source": [
    "## A. Upload CSVs to S3 (using boto3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c166a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01436c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies or put them in a requirements.txt file\n",
    "\n",
    "# !pip install sqlalchemy\n",
    "# !pip install sqlalchemy psycopg2-binary pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# Setup\n",
    "bucket_name = \"amdari-etl-bucket\"\n",
    "folder_prefix = \"medoptix/raw/\"\n",
    "local_folder = \"./medoptix_data\"\n",
    "\n",
    "# AWS credentials assumed to be in ~/.aws/credentials or env vars\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Upload all CSVs in folder with ACL: bucket-owner-full-control\n",
    "for file_name in os.listdir(local_folder):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(local_folder, file_name)\n",
    "        s3.upload_file(\n",
    "            Filename=file_path,\n",
    "            Bucket=bucket_name,\n",
    "            Key=folder_prefix + file_name, \n",
    "            ExtraArgs={'ACL': 'bucket-owner-full-control'}  # <-- THIS IS KEY\n",
    "        )\n",
    "        print(f\"✅ Uploaded {file_name} to s3://{bucket_name}/{folder_prefix}{file_name} with full access\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579e8d4",
   "metadata": {},
   "source": [
    "## B. Load from S3 → PostgreSQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "bucket = \"amdari-etl-bucket\"\n",
    "prefix = \"medoptix/raw/\"\n",
    "\n",
    "# Files to download\n",
    "files = [\"patients.csv\", \"clinics.csv\", \"sessions.csv\", \"feedback.csv\", \"dropout_flags.csv\"]\n",
    "\n",
    "# Download files\n",
    "for file in files:\n",
    "    s3.download_file(bucket, prefix + file, file)\n",
    "    print(f\"⬇️ Downloaded {file} from S3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68acda84",
   "metadata": {},
   "source": [
    "# Data Modelling & Defining Foreign Key relationships\n",
    "\n",
    "\n",
    "- patients (PK: patient_id)\n",
    "- sessions (PK: session_id, FK: patient_id)\n",
    "- feedback (PK: feedback_id, FK: session_id)\n",
    "\n",
    "\n",
    "TASK 1 - Model and Define the relationship for the remaining set of dataset\n",
    " - clinics\n",
    " - dropout_flags\n",
    " - intervention.csv\n",
    "\n",
    "TASK 2 - Create Schema for these dataset and upload then into postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd463347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fbcd7e9",
   "metadata": {},
   "source": [
    "## C. Upload Data → PostgreSQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507de508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (recommended for security)\n",
    "load_dotenv()\n",
    "\n",
    "def get_db_engine():\n",
    "    \"\"\"Create and return a SQLAlchemy engine with proper connection string\"\"\"\n",
    "    # Construct connection string from environment variables\n",
    "    db_url = (\n",
    "        f\"postgresql://{''}:{''}@\"\n",
    "        f\"{''}:{''}/{''}?\"\n",
    "    )\n",
    "    return create_engine(db_url)\n",
    "\n",
    "def upload_data():\n",
    "    # Step 1: Read CSV files\n",
    "    patients = pd.read_csv(\"C:/Users/Muham/Downloads/Medoptix_Demo/medoptix_data/processed/patients.csv\")\n",
    "    sessions = pd.read_csv(\"C:/Users/Muham/Downloads/Medoptix_Demo/medoptix_data/processed/sessions.csv\")\n",
    "    feedback = pd.read_csv(\"C:/Users/Muham/Downloads/Medoptix_Demo/medoptix_data/processed/feedback.csv\")\n",
    "\n",
    "    # Step 2: Create database engine\n",
    "    engine = get_db_engine()\n",
    "\n",
    "    # Step 3: Upload in referential order with error handling\n",
    "    with engine.begin() as connection:  # Automatically handles transactions\n",
    "        # Chunk size for large datasets (adjust as needed)\n",
    "        chunk_size = 1000\n",
    "\n",
    "        # Upload patients table\n",
    "        patients.to_sql(\n",
    "            \"patients\", \n",
    "            connection, \n",
    "            if_exists=\"append\", \n",
    "            index=False,\n",
    "            chunksize=chunk_size,\n",
    "            method='multi'  # Faster for bulk inserts\n",
    "        )\n",
    "\n",
    "        # Upload sessions table\n",
    "        sessions.to_sql(\n",
    "            \"sessions\", \n",
    "            connection, \n",
    "            if_exists=\"append\", \n",
    "            index=False,\n",
    "            chunksize=chunk_size,\n",
    "            method='multi'\n",
    "        )\n",
    "\n",
    "        # Upload feedback table\n",
    "        feedback.to_sql(\n",
    "            \"feedback\", \n",
    "            connection, \n",
    "            if_exists=\"append\", \n",
    "            index=False,\n",
    "            chunksize=chunk_size,\n",
    "            method='multi'\n",
    "        )\n",
    "\n",
    "    print(\"✅ Data uploaded successfully with relationships intact.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upload_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef694993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c469388",
   "metadata": {},
   "source": [
    "## D. Read Data → PostgreSQL - (Prepare data for EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_db_engine():\n",
    "    \"\"\"Create and return a SQLAlchemy engine with proper connection string\"\"\"\n",
    "    # Construct connection string from environment variables\n",
    "    db_url = (\n",
    "        f\"postgresql://{''}:{''}@\"\n",
    "        f\"{''}:{''}/{''}?\"\n",
    "    )\n",
    "    return create_engine(db_url)\n",
    "\n",
    "\n",
    "engine = get_db_engine()\n",
    "\n",
    "\n",
    "# Query to fetch data from the tables\n",
    "patients_query = \"SELECT * FROM patients\"\n",
    "sessions_query = \"SELECT * FROM sessions\"\n",
    "feedback_query = \"SELECT * FROM feedback\"\n",
    "\n",
    "# Load data into pandas DataFrame\n",
    "patients_df = pd.read_sql(patients_query, engine)\n",
    "sessions_df = pd.read_sql(sessions_query, engine)\n",
    "feedback_df = pd.read_sql(feedback_query, engine)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac68f4",
   "metadata": {},
   "source": [
    "### D1- Display basic info and summary statistics for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patients\n",
    "print(\"Patients Data Overview\")\n",
    "patients_df.head(2)\n",
    "patients_df.info()\n",
    "patients_df.describe()\n",
    "\n",
    "# Sessions\n",
    "print(\"Sessions Data Overview\")\n",
    "sessions_df.head(2)\n",
    "sessions_df.info()\n",
    "sessions_df.describe()\n",
    "\n",
    "\n",
    "# Feedback\n",
    "print(\"Feedback Data Overview\")\n",
    "feedback_df.head()\n",
    "feedback_df.info()\n",
    "feedback_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2a439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35dca40e",
   "metadata": {},
   "source": [
    "## E. EDA → (Exploratory Data Analysis)\n",
    "\n",
    "Steps in EDA\n",
    " - Data Exploration\n",
    "    - Head()\n",
    "    - info()\n",
    "    - describe()\n",
    " - Data Cleaning \n",
    "    - Identifying & Handling Missing Values\n",
    "    - Identifying & Handling Duplicates\n",
    " - Data Transformation (Feature Engineering)\n",
    "   - Standardizing data types (text, date, replacing values)\n",
    "   - Log Transform (for highly skewed data e.g., income, prices)\n",
    "   - Binning (Convert continuous variables into categorical bins e.g., age ranges)\n",
    "   - Encoding (Convert categorical variables into numerical format (e.g., One-Hot, Label Encoding))\n",
    " - Data Distributions\n",
    "    - Histograms for distributions of numerical features (age, bmi etc)- Also to check if the features are normally distributed or skewed (e.g., pain level or age should be somewhat evenly distributed just like we discovered in the last drop-in session).\n",
    "    - Boxplots to detect outliers & handle them.\n",
    "    - Pairplots to visualize relationships between multiple features.\n",
    " -  Correlation Analysis (dentify relationships between variables that may help in predictive modeling)\n",
    "    - Correlation Matrix to understand feature dependencies.\n",
    "    - Heatmap for visual representation of correlations.\n",
    " - Understanding Dropout Behavior\n",
    "    - investigate the correlation between features and patient dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23866072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d71349a",
   "metadata": {},
   "source": [
    "#### E-2: Data Cleaning (Handling Missing Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Check for Missing Values in All Tables\n",
    "print(\"\\nMissing Values in Patients Data:\")\n",
    "print(patients_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Values in Sessions Data:\")\n",
    "print(sessions_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Values in Feedback Data:\")\n",
    "print(feedback_df.isnull().sum())\n",
    "\n",
    "# 6. Clean Missing Values\n",
    "# Example of filling missing values for categorical and numerical data\n",
    "patients_df['chronic_cond'] = patients_df['chronic_cond'].fillna(patients_df['chronic_cond'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70110f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if null values has been properly handled\n",
    "patients_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6fbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb59a240",
   "metadata": {},
   "source": [
    "#### E-3 - TASK 3 (Data Cleaning)\n",
    "- Identifying & Handling Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "1   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e8c475",
   "metadata": {},
   "source": [
    "# F. Data transformation (Feature Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cefa47",
   "metadata": {},
   "source": [
    "#### F-1 Standardizing data types (Signup_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ed984",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_df['signup_date'] = pd.to_datetime(patients_df['signup_date'])\n",
    "sessions_df['date'] = pd.to_datetime(sessions_df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76792d",
   "metadata": {},
   "source": [
    "#### F-2 Binning (putting the age into categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aef8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Add age group column for segmentation\n",
    "bins = [0, 18, 35, 55, np.inf]\n",
    "labels = ['0-18', '19-35', '36-55', '55+']\n",
    "patients_df['age_group'] = pd.cut(patients_df['age'], bins=bins, labels=labels)\n",
    "patients_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e14b26",
   "metadata": {},
   "source": [
    "F-3 : TASK 4 (Data Transformation)\n",
    "- Log Transform (for highly skewed data e.g., income, prices)\n",
    "- Encoding (Convert categorical variables into numerical format (e.g., One-Hot, Label Encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a7714",
   "metadata": {},
   "source": [
    "# G. Data Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e89dbb",
   "metadata": {},
   "source": [
    "#### G-1  Histograms for distributions of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histogram for distribution of pain level\n",
    "sns.histplot(sessions_df['pain_level'], kde=True)\n",
    "plt.title('Distribution of Pain Level')\n",
    "plt.show()\n",
    "\n",
    "# Age distribution for patients (histogram)\n",
    "sns.histplot(patients_df['age'], kde=True)\n",
    "plt.title('Age Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3124b96f",
   "metadata": {},
   "source": [
    "#### G-2  TASK 5 (Data Distribution)\n",
    "- Boxplots to detect outliers & handle them.\n",
    "- Pairplots to visualize relationships between multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d425e653",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2808bf8d",
   "metadata": {},
   "source": [
    "# H. CORRELATION ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79920c",
   "metadata": {},
   "source": [
    "#### H-1 TASK 6 (Correlation Analysis)\n",
    "\n",
    "-  Correlation Analysis (dentify relationships between variables that may help in predictive modeling)\n",
    "- Correlation Matrix to understand feature dependencies.\n",
    "- Heatmap for visual representation of correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b5db5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
